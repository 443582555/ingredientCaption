{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yifu/anaconda3/envs/pytorch2.7/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "#Python Packages.\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Torch Packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "# Import the Model.\n",
    "from model import image_ingredient\n",
    "from decoder import HierRNN\n",
    "from data_loader import ImageLoader\n",
    "import pdb\n",
    "# Packages which will help Loading the data.\n",
    "import pickle\n",
    "import lmdb\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_PATH = '/home/yifu/Documents/Mycode/python/hierarchicalRNN/jasha/'\n",
    "# GIve the path for the LMDB files that were created.\n",
    "DATA_PATH = '/home/yifu/Documents/Mycode/python/hierarchicalRNN/jasha/lmdb/'\n",
    "WORKERS = 0\n",
    "BATCH_SIZE = 40\n",
    "VAL_FREQ = 1\n",
    "START_EPOCH = 0\n",
    "TOTAL_EPOCH = 500\n",
    "\n",
    "\n",
    "#batch_size = 50 # Being support batch_size\n",
    "num_boxes = 50 # number of Detected regions in each image\n",
    "feats_dim = 4096 # feature dimensions of each regions\n",
    "project_dim = 1024 # project the features to one vector, which is 1024 dimensions\n",
    "\n",
    "sentRNN_lstm_dim = 1024 # the sentence LSTM hidden units\n",
    "sentRNN_FC_dim = 1024 # the fully connected units\n",
    "wordRNN_lstm_dim = 512 # the word LSTM hidden units\n",
    "word_embed_dim = 1024 # the learned embedding vectors for the words\n",
    "\n",
    "S_max = 3\n",
    "N_max = 20\n",
    "T_stop = 0.5\n",
    "\n",
    "\n",
    "#500\n",
    "n_epochs = 40\n",
    "learning_rate = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import torch.nn.functional as F\n",
    "\n",
    "sentRNN_lstm_dim = 512 # the sentence LSTM hidden units\n",
    "sentRNN_FC_dim = 1024 # the fully connected units\n",
    "wordRNN_lstm_dim = 512 # the word LSTM hidden units\n",
    "word_embed_dim = 1024 # the learned embedding vectors for the words\n",
    "\n",
    "class HierRNN(nn.Module):\n",
    "    def __init__(self, n_words,\n",
    "                       batch_size,\n",
    "                       num_boxes,\n",
    "                       feats_dim,\n",
    "                       project_dim,\n",
    "                       sentRNN_lstm_dim,\n",
    "                       sentRNN_FC_dim,\n",
    "                       wordRNN_lstm_dim,\n",
    "                       S_max,\n",
    "                       N_max,\n",
    "                       word_embed_dim,bias_init_vector=None):\n",
    "        \"\"\"Set the hyper-parameters and build the layers.\"\"\"\n",
    "        super(HierRNN, self).__init__()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        #embedding\n",
    "        self.S_max = S_max\n",
    "        self.N_max = N_max\n",
    "        self.embed = nn.Embedding(n_words, word_embed_dim)\n",
    "        self.sentLSTM = nn.LSTM(project_dim, sentRNN_lstm_dim,batch_first=True)\n",
    "        \n",
    "        self.word_LSTM = nn.LSTM(word_embed_dim,wordRNN_lstm_dim,2,batch_first=True)\n",
    "        \n",
    "        self.sent_FC = nn.Linear(sentRNN_lstm_dim,sentRNN_FC_dim)\n",
    "        self.sent_FC2  = nn.Linear(sentRNN_FC_dim,1024)\n",
    "        self.word_FC = nn.Linear(wordRNN_lstm_dim,n_words)\n",
    "        self.binarize = nn.Linear(sentRNN_lstm_dim,2)\n",
    "        self.m = nn.Softmax(dim=2)\n",
    "#         self.sentLstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "#         self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "#         self.init_weights()\n",
    "\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # No matter whether CUDA is used, the returned variable will have the same type as x.\n",
    "        weight = next(self.parameters()).data\n",
    "        return Variable(weight.new(1, batch_size, sentRNN_lstm_dim).zero_())\n",
    "        \n",
    "    def forward(self, features, captions):\n",
    "        \"\"\"Decode image feature vectors and generates captions.\"\"\"\n",
    "        #feats  = \n",
    "        sent_state = self.init_hidden(self.batch_size)\n",
    "        \n",
    "#         features = self.fcregionPool(features)\n",
    "#         features.transpose_(1,2)\n",
    "        #print features\n",
    "        features = features.view(self.batch_size ,1,-1)\n",
    "        temp_distribution= Variable(torch.Tensor()).cuda()\n",
    "        final_output = Variable(torch.Tensor()).cuda()\n",
    "\n",
    "        for i in range(0,self.S_max):\n",
    "            sent_output,sent_state = self.sentLSTM(features)\n",
    "            \n",
    "            hidden1 = F.relu(self.sent_FC(sent_output))\n",
    "            sent_topic_vec = F.relu(self.sent_FC2(hidden1))\n",
    "            \n",
    "            sentRNN_binary = self.binarize(sent_output)\n",
    "            #sentRNN_binary = sentRNN_binary\n",
    "            h1 = sent_topic_vec[:,:,0:512].transpose_(0,1)\n",
    "            c1 = sent_topic_vec[:,:,512:].transpose_(0,1)\n",
    "            h1= torch.cat((h1,h1),0)\n",
    "            c1= torch.cat((c1,c1),0)\n",
    "            #[000001111]\n",
    "            temp_distribution = torch.cat((temp_distribution,sentRNN_binary),1)\n",
    "            \n",
    "            ############not sure \n",
    "            temp_output=Variable(torch.Tensor().long()).cuda()\n",
    "#             for j in range(0, self.N_max):\n",
    "                \n",
    "#                 current_embed = self.embed(captions[:,i,j])\n",
    "#                 print current_embed.size()\n",
    "                \n",
    "#                 current_embed=current_embed.unsqueeze(1)\n",
    "#                 word_output,word_state = self.word_LSTM(current_embed,(h1,c1))\n",
    "#                 word_output = self.m(self.word_FC(word_output))\n",
    "#                 temp_output = torch.cat((temp_output,word_output),1)\n",
    "            print captions.shape\n",
    "            current_embed = self.embed(captions[:,i,:])\n",
    "            #print(current_embed.size())\n",
    "        \n",
    "            #need to do pack padded sequence. \n",
    "            \n",
    "            word_output,word_state = self.word_LSTM(current_embed,(h1,c1))\n",
    "            #convert it back\n",
    "            \n",
    "            word_output = self.word_FC(word_output)\n",
    "            word_output= word_output.unsqueeze(1)\n",
    "            final_output = torch.cat((final_output,word_output),1)\n",
    "        #print final_output.size()\n",
    "        return temp_distribution,final_output\n",
    "    #input dimemtion [batch*1024]\n",
    "    def sample(self, features,captionMask=None,states=None):\n",
    "        \"\"\"Decode image feature vectors and generates captions.\"\"\"\n",
    "        #feats  = \n",
    "       \n",
    "        #sent_state = self.init_hidden(self.batch_size)\n",
    "        \n",
    "#         features = self.fcregionPool(features)\n",
    "#         features.transpose_(1,2)\n",
    "        #print features\n",
    "    \n",
    "    \n",
    "        features = features.view(self.batch_size ,1,-1).LongTensor.cuda()\n",
    "        temp_distribution= Variable(torch.Tensor()).cuda()\n",
    "        final_output = Variable(torch.Tensor()).cuda()\n",
    "        for i in range(0,self.S_max):\n",
    "            \n",
    "            sent_output,sent_state = self.sentLSTM(features)\n",
    "            hidden1 = F.relu(self.sent_FC(sent_output))\n",
    "            sent_topic_vec = F.relu(hidden1.LongTensor())\n",
    "            \n",
    "#             sentRNN_binary = self.binarize(sent_output)\n",
    "#             sentRNN_binary = self.m(sentRNN_binary)\n",
    "            h1 = sent_topic_vec[:,:,0:512].transpose_(0,1)\n",
    "            c1 = sent_topic_vec[:,:,512:].transpose_(0,1)\n",
    "            h1= torch.cat((h1,h1),0)\n",
    "            c1= torch.cat((c1,c1),0)\n",
    "            #[000001111]\n",
    "#             temp_distribution = torch.cat((temp_distribution,sentRNN_binary),1)\n",
    "            \n",
    "            temp_output=[]\n",
    "            c_embed = Variable(torch.Tensor()).cuda()\n",
    "            for j in range(0, self.N_max):\n",
    "                #current_embed = self.embed(captions[:,i,j])\n",
    "                #print current_embed.size()\n",
    "                if j==0:\n",
    "                    inputs = Variable(torch.zeros([self.batch_size,1,1]))\n",
    "                    current_embed = self.embed(inputs)\n",
    "                else:\n",
    "                    current_embed = c_embed\n",
    "                #current_embed=current_embed.unsqueeze(1)\n",
    "                word_output,word_state = self.word_LSTM(current_embed,(h1,c1))\n",
    "                word_output = self.m(self.word_FC(word_output.squeeze(1)))\n",
    "                _, predicted = outputs.max(1) \n",
    "                \n",
    "                temp_output.append(predicted)\n",
    "                c_embed = self.embed(predicted)\n",
    "                #inputs = inputs.unsqueeze(1) \n",
    "            \n",
    "            temp_output= temp_output.unsqueeze(1)\n",
    "            final_output = torch.cat((final_output,temp_output),1)\n",
    "        #print final_output.size()\n",
    "        return temp_distribution,final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getIdx():\n",
    "    ixtoword = {}\n",
    "    wordtoix = {}\n",
    "    ingr_vocab = {}\n",
    "    #Vocab.text is the pre-processed corpus of all the instructions.\n",
    "    with open('../files/vocab.txt') as f_vocab:\n",
    "        #wordtoix a dict mapped to the index values. Key is the word and value is the index value.\n",
    "        #ingr_vocab = {w.rstrip(): i+2 for i, w in enumerate(f_vocab)} # +1 for lua\n",
    "        #ingr_vocab['</i>'] = 1\n",
    "        #print (ingr_vocab)\n",
    "        for i, w in enumerate(f_vocab):\n",
    "            word = w.rstrip()\n",
    "            ixtoword[i+5] = word\n",
    "            wordtoix[w.rstrip()] = i+5\n",
    "        wordtoix['</i>'] = 4\n",
    "        wordtoix['<bos>'] = 0\n",
    "        wordtoix['<eos>'] = 1\n",
    "        wordtoix['<pad>'] = 2\n",
    "        wordtoix['<unk>'] = 3\n",
    "        ixtoword[0] = '<bos>'\n",
    "        ixtoword[1] = '<eos>'\n",
    "        ixtoword[2] = '<pad>'\n",
    "        ixtoword[3] = '<unk>'\n",
    "        ixtoword[4] = '</i>'\n",
    "    return wordtoix,ixtoword\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (4) : unspecified launch failure at /opt/conda/conda-bld/pytorch_1524580938250/work/aten/src/THC/generic/THCTensorCopy.c:20",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-7a175b951f28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mword2idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetIdx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m#model = image_ingredient().cuda()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoderCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;31m#decoder=DecoderRNN(1024, 512, len(word2idx), 2).cuda()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m decoder = HierRNN(n_words = len(word2idx),\n",
      "\u001b[0;32m/home/yifu/anaconda3/envs/pytorch2.7/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \"\"\"\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yifu/anaconda3/envs/pytorch2.7/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yifu/anaconda3/envs/pytorch2.7/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yifu/anaconda3/envs/pytorch2.7/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    180\u001b[0m                 \u001b[0;31m# Tensors stored in modules are graph leaves, and we don't\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0;31m# want to create copy nodes, so we have to unpack the data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m                 \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                     \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yifu/anaconda3/envs/pytorch2.7/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \"\"\"\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (4) : unspecified launch failure at /opt/conda/conda-bld/pytorch_1524580938250/work/aten/src/THC/generic/THCTensorCopy.c:20"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        \"\"\"Load the pretrained ResNet-152 and replace top fc layer.\"\"\"\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet152(pretrained=True)\n",
    "        modules = list(resnet.children())[:-1]      # delete the last fc layer.\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.linear = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        \"\"\"Extract feature vectors from input images.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            features = self.resnet(images)\n",
    "        features = features.reshape(features.size(0), -1)\n",
    "        features = self.bn(self.linear(features))\n",
    "        return features\n",
    "    \n",
    "    \n",
    "word2idx = getIdx()\n",
    "#model = image_ingredient().cuda()\n",
    "model = EncoderCNN(1024).cuda()\n",
    "#decoder=DecoderRNN(1024, 512, len(word2idx), 2).cuda()\n",
    "decoder = HierRNN(n_words = len(word2idx),\n",
    "                                          batch_size = BATCH_SIZE,\n",
    "                                          num_boxes = num_boxes,\n",
    "                                          feats_dim = feats_dim,\n",
    "                                          project_dim = project_dim,\n",
    "                                          sentRNN_lstm_dim = sentRNN_lstm_dim,\n",
    "                                          sentRNN_FC_dim = sentRNN_FC_dim,\n",
    "                                          wordRNN_lstm_dim = wordRNN_lstm_dim,\n",
    "                                          S_max = S_max,\n",
    "                                          N_max = N_max,\n",
    "                                          word_embed_dim = word_embed_dim\n",
    "                                            #bias_init_vector = bias_init_vector\n",
    "                     ).cuda()\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "params = list(decoder.parameters()) + list(model.linear.parameters()) + list(model.bn.parameters())\n",
    "optimizer = torch.optim.Adam(params,lr=learning_rate)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        ImageLoader(IMG_PATH,\n",
    "            transforms.Compose([\n",
    "            transforms.Scale(256), # rescale the image keeping the original aspect ratio\n",
    "            transforms.CenterCrop(256), # we get only the center of that rescaled\n",
    "            transforms.RandomCrop(224), # random crop within the center crop\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]),data_path=DATA_PATH,partition='train'),\n",
    "        batch_size=BATCH_SIZE, shuffle=True,\n",
    "        num_workers=WORKERS, pin_memory=True)\n",
    "print ('Training loader prepared.')\n",
    "totalloss=[]\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(3):\n",
    "        #for i, (images, captions, lengths) in enumerate(data_loader):\n",
    "        for i, (input, target) in enumerate(train_loader):\n",
    "            # Set mini-batch dataset\n",
    "            images = Variable(input[0]).cuda()\n",
    "            x = Variable(input[4][:,:,0:20]).cuda()\n",
    "            \n",
    "#             seq_lengths = input[8]-1\n",
    "#             #batch_in = Variable(captions)\n",
    "#             #############\n",
    "#             x_sort_idx = np.argsort(-seq_lengths)\n",
    "#             x_unsort_idx = torch.LongTensor(np.argsort(x_sort_idx))\n",
    "#             x_len = seq_lengths[x_sort_idx]\n",
    "#             x = x[torch.LongTensor(x_sort_idx)]\n",
    "      \n",
    "            \"\"\"pack\"\"\"\n",
    " \n",
    "#             targets = pack_padded_sequence(x, x_len, batch_first=True)[0]\n",
    "            \n",
    "            # Forward, backward and optimize\n",
    "            #encoder_output = model(Variable(input[0]).cuda(), Variable(input[1]).cuda(), Variable(input[2]).cuda())\n",
    "            encoder_output = model(images).float()\n",
    "#             encoder_output = encoder_output[torch.LongTensor(x_sort_idx)]\n",
    "            outputs = decoder(encoder_output, x)\n",
    "            print output\n",
    "            loss = criterion(outputs, targets)\n",
    "            decoder.zero_grad()\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "#             out = torch.nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)  # (sequence, lengths)\n",
    "#             out = out[0]\n",
    "#             print out\n",
    "#             out = out[x_unsort_idx]\n",
    "            \n",
    "            \n",
    "           \n",
    "            # Print log info\n",
    "            if i % 100 == 0:\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Perplexity: {:5.4f}'\n",
    "                      .format(epoch, 100, i, total_step, loss.item(), np.exp(loss.item()))) \n",
    "torch.save(decoder.state_dict(), os.path.join(\n",
    "                    \"simpleModel/\", 'decoder-{}.ckpt'.format(epoch+1)))\n",
    "torch.save(model.state_dict(), os.path.join(\n",
    "                    \"simpleModel/\", 'encoder-{}.ckpt'.format(epoch+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                             (0.229, 0.224, 0.225))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path, transform=None):\n",
    "    image = Image.open(image_path)\n",
    "    image = image.resize([224, 224], Image.LANCZOS)\n",
    "    \n",
    "    if transform is not None:\n",
    "        image = transform(image).unsqueeze(0)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = load_image(\"testFiles/5.jpg\", transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_tensor = image.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval()\n",
    "feature = model(image_tensor)\n",
    "decoder = decoder.eval()\n",
    "sampled_ids = decoder.sample(feature)\n",
    "sampled_ids = sampled_ids[0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,idx2word = getIdx()\n",
    "sampled_caption = []\n",
    "for word_id in sampled_ids:\n",
    "    word = idx2word[word_id]\n",
    "    sampled_caption.append(word)\n",
    "    if word == '<end>':\n",
    "        break\n",
    "sentence = ' '.join(sampled_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch2.7]",
   "language": "python",
   "name": "conda-env-pytorch2.7-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
