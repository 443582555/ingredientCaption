{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import copy\n",
    "import pickle\n",
    "import lmdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Ingredients vocab.\n"
     ]
    }
   ],
   "source": [
    "print('Loading Ingredients vocab.')\n",
    "# Making the dict of the vocab of the ingredients.\n",
    "ixtoword = {}\n",
    "wordtoix = {}\n",
    "ingr_vocab = {}\n",
    "#Vocab.text is the pre-processed corpus of all the instructions.\n",
    "with open('../files/vocab.txt') as f_vocab:\n",
    "    #wordtoix a dict mapped to the index values. Key is the word and value is the index value.\n",
    "    #ingr_vocab = {w.rstrip(): i+2 for i, w in enumerate(f_vocab)} # +1 for lua\n",
    "    #ingr_vocab['</i>'] = 1\n",
    "    #print (ingr_vocab)\n",
    "    for i, w in enumerate(f_vocab):\n",
    "        word = w.rstrip()\n",
    "        ixtoword[i+5] = word\n",
    "        wordtoix[w.rstrip()] = i+5\n",
    "       # print(w)\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordtoix['</i>'] = 4\n",
    "\n",
    "wordtoix['<bos>'] = 0\n",
    "wordtoix['<eos>'] = 1\n",
    "wordtoix['<pad>'] = 2\n",
    "wordtoix['<unk>'] = 3\n",
    "\n",
    "ixtoword[0] = '<bos>'\n",
    "ixtoword[1] = '<eos>'\n",
    "ixtoword[2] = '<pad>'\n",
    "ixtoword[3] = '<unk>'\n",
    "ixtoword[4] = '</i>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYER_1_PATH = '../layer1.json'\n",
    "LAYER_2_PATH = '../layer2.json'\n",
    "DET_INGRS_PATH = '../det_ingrs.json'\n",
    "CLASSES1M_PKL_PATH = '../classes1M.pkl'\n",
    "TRAIN_LMDB_PATH = '/home/yifu/Documents/Mycode/python/hierarchicalRNN/jasha/lmdb/train_lmdb'\n",
    "VALID_LMDB_PATH = '/home/yifu/Documents/Mycode/python/hierarchicalRNN/jasha/lmdb/val_lmdb'\n",
    "TEST_LMDB_PATH =  '/home/yifu/Documents/Mycode/python/hierarchicalRNN/jasha/lmdb/test_lmdb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH_INGRS = 20\n",
    "MAX_NUM_IMGS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open the det_ingrs.json.\n",
    "json_det_ingrs = open(DET_INGRS_PATH).read() \n",
    "# data set for all the ingredients. Store the content in dataset_det_ingrs\n",
    "dataset_det_ingrs = json.loads(json_det_ingrs) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open the layer1.json file\n",
    "json_layer_1 = open(LAYER_1_PATH).read()\n",
    "dataset_layer_1 = json.loads(json_layer_1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Open the layer2.json file\n",
    "json_layer_2 = open(LAYER_2_PATH).read()\n",
    "dataset_layer_2 = json.loads(json_layer_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(CLASSES1M_PKL_PATH,'rb') as f:\n",
    "    class_dict = pickle.load(f)\n",
    "    id2class = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Still in doubt how this function work, but basically it return the combined .json files.\n",
    "def merge(layers):\n",
    "    base = layers[0]\n",
    "    entries_by_id = {entry['id']: entry for entry in base}\n",
    "    for layer in layers[1:]:\n",
    "        for entry in layer:\n",
    "            base_entry = entries_by_id.get(entry['id'])\n",
    "            if not base_entry:\n",
    "                continue\n",
    "            base_entry.update(entry)\n",
    "    return base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the data set into one list. \n",
    "dataset = merge([dataset_layer_1,dataset_layer_2,dataset_det_ingrs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessSencentes(listOfSentences,S_max,N_max,word2idx):\n",
    "    numOfSentences = len(listOfSentences)\n",
    "    numForEachSentences = []\n",
    "    sentences=[]\n",
    "    for i in listOfSentences:\n",
    "        sentences.append(i['text'])\n",
    "    if '' in sentences:\n",
    "        sentences.remove('')\n",
    "    if numOfSentences > S_max:\n",
    "        numOfSentences = S_max  \n",
    "    img_num_distribution = np.zeros([S_max], dtype=np.int32)\n",
    "    img_num_distribution[numOfSentences-1:] = 1\n",
    "    img_captions_matrix = np.ones([S_max, N_max+1], dtype=np.int32) * 2 # zeros([6, 50])\n",
    "    img_caption_one_matrix = np.ones([100],dtype=np.int32)*2\n",
    "    catSentence = \"\"\n",
    "    for i in sentences:\n",
    "        i = i.replace(',', ' ,')\n",
    "        if i[0] == ' ' and i[1] != ' ':\n",
    "            i = i[1:]\n",
    "        elif i[0] == ' ' and i[1] == ' ' and i[2] != ' ':\n",
    "            i = i[2:]\n",
    "\n",
    "        if i[-1] == '.':\n",
    "            i = i[0:-1]\n",
    "        elif i[-1] == ' ' and i[-2] == '.':\n",
    "            i = i[0:-2]\n",
    "        catSentence +=  i\n",
    "    catSentence = '<bos> ' + catSentence + ' <eos>'\n",
    "    oneSentenceLength =0\n",
    "    for idx, word in enumerate(catSentence.lower().split(' ')):\n",
    "        oneSentenceLength+=1\n",
    "        if idx == 100:\n",
    "                break        # the number of sentences is img_num_sents\n",
    "        if word in word2idx:\n",
    "                img_caption_one_matrix[idx] = word2idx[word]\n",
    "        else:\n",
    "                img_caption_one_matrix[idx] = word2idx['<unk>']\n",
    "        # because we treat the ',' as a word\n",
    "\n",
    "    for idx, img_sent in enumerate(sentences):\n",
    "        # Because I have preprocess the paragraph_v1.json file in VScode before,\n",
    "        # and I delete all the 2, 3, 4...bankspaces\n",
    "        # so, actually, the 'elif' code will never run\n",
    "        img_sent = img_sent.replace(',', ' ,')\n",
    "        \n",
    "        if img_sent[0] == ' ' and img_sent[1] != ' ':\n",
    "            img_sent = img_sent[1:]\n",
    "        elif img_sent[0] == ' ' and img_sent[1] == ' ' and img_sent[2] != ' ':\n",
    "            img_sent = img_sent[2:]\n",
    "\n",
    "        # Be careful the last part in a sentence, like this:\n",
    "        # '...world.'\n",
    "        # '...world. '\n",
    "        if img_sent[-1] == '.':\n",
    "            img_sent = img_sent[0:-1]\n",
    "        elif img_sent[-1] == ' ' and img_sent[-2] == '.':\n",
    "            img_sent = img_sent[0:-2]\n",
    "            \n",
    "        mlength=0\n",
    "        \n",
    "        # Last, we add the <bos> and the <eos> in each sentences\n",
    "        img_sent = '<bos> ' + img_sent + ' <eos>'\n",
    "\n",
    "        # translate each word in a sentence into the unique number in word2idx dict\n",
    "        # when we meet the word which is not in the word2idx dict, we use the mark: <unk>\n",
    "        for idy, word in enumerate(img_sent.lower().split(' ')):\n",
    "            # because the biggest number of words in a sentence is N_max, here is 50\n",
    "            if idy == N_max:\n",
    "                break\n",
    "\n",
    "            if word in word2idx:\n",
    "                img_captions_matrix[idx, idy] = word2idx[word]\n",
    "            else:\n",
    "                img_captions_matrix[idx, idy] = word2idx['<unk>']\n",
    "            mlength = mlength+1\n",
    "        numForEachSentences.append(mlength)\n",
    "\n",
    "    # Pay attention, the value type 'img_name' here is NUMBER, I change it to STRING type\n",
    "    return [img_num_distribution, img_captions_matrix,numOfSentences,numForEachSentences,img_caption_one_matrix,oneSentenceLength]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b ,c,d,e,f=preprocessSencentes(dataset[1]['instructions'],10,20,wordtoix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{u'text': u'Cook macaroni according to package directions; drain well.'},\n",
       " {u'text': u'Cold.'},\n",
       " {u'text': u'Combine macaroni, cheese cubes, celery, green pepper and pimento.'},\n",
       " {u'text': u'Blend together mayonnaise or possibly salad dressing, vinegar, salt and dill weed; add in to macaroni mix.'},\n",
       " {u'text': u'Toss lightly.'},\n",
       " {u'text': u'Cover and refrigeratewell.'},\n",
       " {u'text': u'Serve salad in lettuce lined bowl if you like.'},\n",
       " {u'text': u'Makes 6 servings.'}]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]['instructions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,   42, 1094,  482,   11,  438,    3,  243,    3, 1094,    7,\n",
       "         72,  518,    7,  377,    7,  399,   48,    9,    3,   65,  554,\n",
       "         18,  378,  339,  292,    7,  300,    7,   36,    9,  974,    3,\n",
       "         34,   12,   11, 1094,    3,    3,    9,    3,  339,   12,  625,\n",
       "        708,   24,   91,   61,    3,  194,  838,    1,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2], dtype=int32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Won't implicitly convert Unicode to bytes; use .encode()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-e46b9f728bf9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtxn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mtxn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserialized_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# keys to be saved in a pickle file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Won't implicitly convert Unicode to bytes; use .encode()"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "\n",
    "for i,entry in enumerate(dataset):\n",
    "    \n",
    "    # Get the list containg the index value of the ingredients from the vocab.text\n",
    "    ingr_detections = detect_ingrs(entry, wordtoix)\n",
    "    #Length of the ingredients in single recipe.\n",
    "    length_ingrs = len(ingr_detections)\n",
    "    \n",
    "    imgs = entry.get('images')\n",
    "    if imgs:\n",
    "        count += 1\n",
    "    if length_ingrs >= MAX_LENGTH_INGRS or length_ingrs == 0 or not imgs:\n",
    "        continue\n",
    "    #Make a constant length list. and store the content of the variable length ingredients list in it.\n",
    "    # So that each ingredient has same length vec.\n",
    "    ingr_vec = np.zeros((MAX_LENGTH_INGRS), dtype='uint16')\n",
    "    ingr_vec[:length_ingrs] = ingr_detections\n",
    "    instr_vec_sent, instr_vec_sent_word = preprocessSencentes(dataset[i]['instructions'],30,20,wordtoix)\n",
    "    \n",
    "    partition = entry['partition']\n",
    "    \n",
    "    serialized_sample = pickle.dumps( {'ingrs':ingr_vec,\n",
    "        'classes':class_dict[entry['id']]+1, 'imgs':imgs[:MAX_NUM_IMGS], 'instruct':[instr_vec_sent, instr_vec_sent_word]} )\n",
    "    \n",
    "    with env[partition].begin(write=True) as txn:\n",
    "        txn.put('{}'.format(entry['id']), serialized_sample)\n",
    "    \n",
    "    # keys to be saved in a pickle file\n",
    "    keys[partition].append(entry['id'])\n",
    "    dummy += 1\n",
    "    if i % 10 == 0:\n",
    "        print (\"{} This much is done\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'white_pepper'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ingr_vec[:length_ingrs]\n",
    "ixtoword[1973]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instr_vec_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0, 2520,    8,  409,   15,    8,   57,   13,   10,  316,   18,\n",
       "         320,    1,    2,    2,    2,    2,    2,    2,    2,    2],\n",
       "       [   0,  196,   13, 4062, 1051,  210,   19,  328,  160,   15,    8,\n",
       "          57,    7,   27,   28,  254,  469,   57, 2033,    1,    2],\n",
       "       [   0,   93,    8,  409,  115,   15,    8,   57,  361,   22,  163,\n",
       "          23,   14,  130,   93,    1,    2,    2,    2,    2,    2],\n",
       "       [   0,  196,  247,   48,   19,    8,  132,  122,  126, 2126,   22,\n",
       "           1,    2,    2,    2,    2,    2,    2,    2,    2,    2],\n",
       "       [   0,   49,   93,    8,  317,  115,   14,  130,   93,    1,    2,\n",
       "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2],\n",
       "       [   2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2],\n",
       "       [   2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2],\n",
       "       [   2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2],\n",
       "       [   2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2],\n",
       "       [   2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2],\n",
       "       [   2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2],\n",
       "       [   2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2],\n",
       "       [   2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2],\n",
       "       [   2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2],\n",
       "       [   2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2],\n",
       "       [   2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2],\n",
       "       [   2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2],\n",
       "       [   2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2],\n",
       "       [   2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2],\n",
       "       [   2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2],\n",
       "       [   2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2],\n",
       "       [   2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2],\n",
       "       [   2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2],\n",
       "       [   2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2],\n",
       "       [   2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2],\n",
       "       [   2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2],\n",
       "       [   2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2],\n",
       "       [   2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2],\n",
       "       [   2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2],\n",
       "       [   2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "           2,    2,    2,    2,    2,    2,    2,    2,    2,    2]],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instr_vec_sent_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k in keys.keys():\n",
    "    with open('/home/yifu/Documents/Mycode/python/hierarchicalRNN/jasha/lmdb/{}_keys.pkl'.format(k),'wb') as f:\n",
    "        pickle.dump(keys[k],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Training samples: %d - Validation samples: %d - Testing samples: %d' % (len(keys['train']),len(keys['val']),len(keys['test'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 'hellop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 4\n",
    "b = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bye\n"
     ]
    }
   ],
   "source": [
    "if b !=4:\n",
    "    print 'Bye'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch2.7]",
   "language": "python",
   "name": "conda-env-pytorch2.7-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
