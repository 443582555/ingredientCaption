{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.legacy as legacy\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchwordemb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Following are the things to be discussed with Adam about Model.\n",
    "\n",
    "H-RNN model to be added. Basically we need a class. On the encoder part to be written. Or it should be the separate file.\n",
    "Separate file is prefered since are doing AutoEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ING_WORD2VEC_DIM = 300\n",
    "ING_RNN_DIM = 300\n",
    "ING_WORD2VEC_PATH = '/data/jtaggar/project/code/files/vocab.bin'\n",
    "IMAGE_MODEL = 'resNet50'\n",
    "IMAGE_LAYER_FINAL = 2048\n",
    "EMBD_DIM = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ingredient_RNN (nn.Module):\n",
    "    def __int__(self):\n",
    "        super(ingredient_RNN, self).__init__()\n",
    "        \n",
    "        self.irnn = nn.LSTM(input_size= ING_WORD2VEC_DIM, hidden_size=ING_RNN_DIM, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        #Get the size of the Vocab.\n",
    "        _, vec = torchwordemb.load_word2vec_bin(ING_WORD2VEC_PATH) # give the vector of size 300\n",
    "        \n",
    "        #Creating the Embedding Matrix and then copy the vectors from Google WORD2VEC model to the embedding variable\n",
    "        self.embs = nn.Embedding(vec.size(0), ING_WORD2VEC_DIM, padding_idx=0) # not sure about the padding idx\n",
    "        \n",
    "        self.embs.weight.data.copy_(vec)\n",
    "    \n",
    "    def forward(self, x, seq_lengths):\n",
    "        # X is the variable, seq_lengths is the length of the ingredients of single Recipe\n",
    "        \n",
    "        ################################\n",
    "        # Following things need to be under stood properly.\n",
    "        #################################\n",
    "        # sort sequence according to the length\n",
    "        sorted_len, sorted_idx = sq_lengths.sort(0, descending=True)\n",
    "        \n",
    "        index_sorted_idx = sorted_idx\\\n",
    "                            .view(-1,1,1).expand_as(x)\n",
    "        sorted_inputs = x.gather(0, index_sorted_idx.long())\n",
    "        \n",
    "        # pack sequence\n",
    "        packed_seq = torch.nn.utils.rnn.pack_padded_sequence(\n",
    "                sorted_inputs, sorted_len.cpu().data.numpy(), batch_first=True)\n",
    "        \n",
    "        # pass it to the rnn\n",
    "        out, hidden = self.irnn(packed_seq)\n",
    "\n",
    "        # unsort the output\n",
    "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
    "\n",
    "        # LSTM\n",
    "        # bi-directional\n",
    "        unsorted_idx = original_idx.view(1,-1,1).expand_as(hidden[0])\n",
    "        \n",
    "        # 2 directions x batch_size x num features, we transpose 1st and 2nd dimension\n",
    "        \n",
    "        output = hidden[0].gather(1,unsorted_idx).transpose(0,1).contiguous()\n",
    "        output = output.view(output.size(0),output.size(1)*output.size(2))\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "embs = nn.Embedding(30566, ING_WORD2VEC_DIM, padding_idx=0)\n",
    "a =  torch.tensor([[[1,2], [2,3],[3,4]],[[1,2], [2,3],[3,4]]])\n",
    "b = [1,23,34,23,23,34,34,5446,56,5,34,0]\n",
    "c = torch.LongTensor(b)\n",
    "import numpy as np\n",
    "d = max(np.nonzero(b)[0]) +1\n",
    "e = torch.autograd.Variable(c)\n",
    "x = embs(e)\n",
    "#d\\.sort(0, descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Following thing need to be understood completely \n",
    "def norm(input, p=2, dim=1, eps=1e-12):\n",
    "    return input / input.norm(p,dim,keepdim=True).clamp(min=eps).expand_as(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class image_ingredient(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(image_ingredient, self).__init__()\n",
    "        if IMAGE_MODEL =='resNet50':\n",
    "\n",
    "            resnet = models.resnet50(pretrained=True)\n",
    "            modules = list(resnet.children())[:-1]  # we do not use the last fc layer.\n",
    "            self.visionMLP = nn.Sequential(*modules)\n",
    "\n",
    "            self.visual_embedding = nn.Sequential(\n",
    "                nn.Linear(IMAGE_LAYER_FINAL, EMBD_DIM),\n",
    "                nn.Tanh(),\n",
    "            )\n",
    "\n",
    "            self.ingredient_embedding = nn.Sequential(\n",
    "                #ING_RNN_DIM is doubtful here.  If it needs to be multiplied or not.\n",
    "                nn.Linear(ING_RNN_DIM*3, EMBD_DIM, EMBD_DIM),\n",
    "                nn.Tanh(),\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            raise Exception('Only resNet50 model is implemented.')\n",
    "\n",
    "        self.ingRNN_    = ingredient_RNN()\n",
    "\n",
    "    def forward(self, x, y1, y2, z1, z2): # we need to check how the input is going to be provided to the model\n",
    "        \n",
    "        #############################\n",
    "        # X is the image matrix.\n",
    "        # Change the input according, z1 and z2 retains the last two parameters for the ing_vec and sequence length\n",
    "        ####################################\n",
    "       \n",
    "        # Ingredient embedding\n",
    "        ingredient_emb = self.ingRNN_(z1,z2)\n",
    "        ingredient_emb = self.ingredient_embedding(ingredient_emb)\n",
    "        \n",
    "        ingredient_emb = norm(ingredient_emb)\n",
    "\n",
    "        # visual embedding\n",
    "        visual_emb = self.visionMLP(x)\n",
    "        visual_emb = visual_emb.view(visual_emb.size(0), -1)\n",
    "        visual_emb = self.visual_embedding(visual_emb)\n",
    "        visual_emb = norm(visual_emb)\n",
    "\n",
    "        output = [visual_emb, ingredient_emb]\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
