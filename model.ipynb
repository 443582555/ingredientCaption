{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.legacy as legacy\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchwordemb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ING_WORD2VEC_DIM = 300\n",
    "ING_RNN_DIM = 300\n",
    "ING_WORD2VEC_PATH = '/home/yifu/Documents/Mycode/python/hierarchicalRNN/jasha/files/vocab.bin'\n",
    "IMAGE_MODEL = 'resNet50'\n",
    "IMAGE_LAYER_FINAL = 2048\n",
    "EMBD_DIM = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ingredient_RNN (nn.Module):\n",
    "    def __int__(self):\n",
    "        super(ingredient_RNN, self).__init__()\n",
    "        \n",
    "        self.irnn = nn.LSTM(input_size= ING_WORD2VEC_DIM, hidden_size=ING_RNN_DIM, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        #Get the size of the Vocab.\n",
    "        _, vec = torchwordemb.load_word2vec_bin(ING_WORD2VEC_PATH) # give the vector of size 300\n",
    "        \n",
    "        #Creating the Embedding Matrix and then copy the vectors from Google WORD2VEC model to the embedding variable\n",
    "        self.embs = nn.Embedding(vec.size(0), ING_WORD2VEC_DIM, padding_idx=0) # not sure about the padding idx\n",
    "        \n",
    "        self.embs.weight.data.copy_(vec)\n",
    "    \n",
    "    def forward(self, x, seq_lengths):\n",
    "        # X is the variable, seq_lengths is the length of the ingredients of single Recipe\n",
    "        \n",
    "        ################################\n",
    "        # Following things need to be under stood properly.\n",
    "        #################################\n",
    "        # sort sequence according to the length\n",
    "        sorted_len, sorted_idx = sq_lengths.sort(0, descending=True)\n",
    "        \n",
    "        index_sorted_idx = sorted_idx\\\n",
    "                            .view(-1,1,1).expand_as(x)\n",
    "        sorted_inputs = x.gather(0, index_sorted_idx.long())\n",
    "        \n",
    "        # pack sequence\n",
    "        packed_seq = torch.nn.utils.rnn.pack_padded_sequence(\n",
    "                sorted_inputs, sorted_len.cpu().data.numpy(), batch_first=True)\n",
    "        \n",
    "        # pass it to the rnn\n",
    "        out, hidden = self.irnn(packed_seq)\n",
    "\n",
    "        # unsort the output\n",
    "        _, original_idx = sorted_idx.sort(0, descending=False)\n",
    "\n",
    "        # LSTM\n",
    "        # bi-directional\n",
    "        unsorted_idx = original_idx.view(1,-1,1).expand_as(hidden[0])\n",
    "        \n",
    "        # 2 directions x batch_size x num features, we transpose 1st and 2nd dimension\n",
    "        \n",
    "        output = hidden[0].gather(1,unsorted_idx).transpose(0,1).contiguous()\n",
    "        output = output.view(output.size(0),output.size(1)*output.size(2))\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-455a81def4bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0membs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30566\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mING_WORD2VEC_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m23\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m34\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m23\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m23\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m34\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m34\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5446\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m56\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m34\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "embs = nn.Embedding(30566, ING_WORD2VEC_DIM, padding_idx=0)\n",
    "a =  torch.tensor([[[1,2], [2,3],[3,4]],[[1,2], [2,3],[3,4]]])\n",
    "b = [1,23,34,23,23,34,34,5446,56,5,34,0]\n",
    "c = torch.LongTensor(b)\n",
    "import numpy as np\n",
    "d = max(np.nonzero(b)[0]) +1\n",
    "e = torch.autograd.Variable(c)\n",
    "x = embs(e)\n",
    "#d\\.sort(0, descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Following thing need to be understood completely \n",
    "def norm(input, p=2, dim=1, eps=1e-12):\n",
    "    return input / input.norm(p,dim,keepdim=True).clamp(min=eps).expand_as(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class image_ingredient(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(image_ingedient, self).__init__()\n",
    "        if IMAGE_MODEL =='resNet50':\n",
    "\n",
    "            resnet = models.resnet50(pretrained=True)\n",
    "            modules = list(resnet.children())[:-1]  # we do not use the last fc layer.\n",
    "            self.visionMLP = nn.Sequential(*modules)\n",
    "\n",
    "            self.visual_embedding = nn.Sequential(\n",
    "                nn.Linear(IMAGE_LAYER_FINAL, EMBD_DIM),\n",
    "                nn.Tanh(),\n",
    "            )\n",
    "\n",
    "            self.ingredient_embedding = nn.Sequential(\n",
    "                #ING_RNN_DIM is doubtful here.  If it needs to be multiplied or not.\n",
    "                nn.Linear(ING_RNN_DIM*3, EMBD_DIM, EMBD_DIM),\n",
    "                nn.Tanh(),\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            raise Exception('Only resNet50 model is implemented.')\n",
    "\n",
    "        self.ingRNN_    = ingredient_RNN()\n",
    "\n",
    "    def forward(self, x, z1, z2): # we need to check how the input is going to be provided to the model\n",
    "        \n",
    "        #############################\n",
    "        # X is the image matrix.\n",
    "        # Change the input according, z1 and z2 retains the last two parameters for the ing_vec and sequence length\n",
    "        ####################################\n",
    "       \n",
    "        # Ingredient embedding\n",
    "        ingredient_emb = self.ingRNN_(z1,z2)\n",
    "        ingredient_emb = self.ingredient_embedding(ingredient_emb)\n",
    "        \n",
    "        ingredient_emb = norm(ingredient_emb)\n",
    "\n",
    "        # visual embedding\n",
    "        visual_emb = self.visionMLP(x)\n",
    "        visual_emb = visual_emb.view(visual_emb.size(0), -1)\n",
    "        visual_emb = self.visual_embedding(visual_emb)\n",
    "        visual_emb = norm(visual_emb)\n",
    "\n",
    "        output = [visual_emb, ingredient_emb]\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = (1 + torch.rand(160) * (1000 - 1)).type(torch.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 506,  518,  367,  999,  547,  322,  219,  951,  702,   60,\n",
       "         554,  343,  556,  103,  906,  612,   68,  673,  686,   54,\n",
       "         562,  255,  963,  486,  365,  156,   99,  637,  903,  223,\n",
       "         473,  271,  809,  846,  324,  678,  464,  969,   37,  440,\n",
       "         563,  177,   53,  858,  900,  833,  567,  396,  516,  561,\n",
       "         228,  201,  790,   28,  330,  164,  122,  433,   74,  996,\n",
       "         166,  302,  973,  441,  987,  414,  621,  102,  392,  415,\n",
       "         450,  558,  621,  645,  100,  847,  400,  640,  279,  564,\n",
       "         352,  987,  427,  475,  542,  481,  956,  801,  711,  907,\n",
       "         128,  989,  888,  157,  190,  877,  577,  422,  810,  514,\n",
       "         779,  128,  940,  227,  189,  951,  475,  738,  100,  937,\n",
       "         773,  804,  704,   64,  628,  428,  468,  324,  803,  468,\n",
       "         690,  829,  763,  337,  349,  671,  668,   91,  985,  250,\n",
       "         407,  954,  288,  855,   83,  472,  785,  662,  350,  481,\n",
       "         823,   58,  341,  666,  438,  196,  344,  643,  878,   97,\n",
       "         539,  351,  720,  559,  940,  107,  184,  450,  716,  133], dtype=torch.int16)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-00440b8cd172>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m160\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "s = 1 + torch.rand(160) * (20 - 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s_l,s_idx = s.sort(0,descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 19,  19,  19,  19,  19,  19,  19,  19,  19,  18,  18,  18,\n",
       "         17,  17,  17,  17,  17,  17,  17,  17,  17,  17,  16,  16,\n",
       "         16,  16,  16,  16,  16,  16,  16,  16,  16,  15,  15,  15,\n",
       "         15,  15,  15,  15,  14,  14,  14,  14,  14,  14,  13,  13,\n",
       "         13,  13,  13,  12,  12,  12,  12,  12,  12,  12,  11,  11,\n",
       "         11,  11,  11,  11,  11,  11,  10,  10,  10,  10,  10,  10,\n",
       "         10,  10,  10,  10,  10,  10,  10,   9,   9,   8,   8,   8,\n",
       "          8,   8,   8,   8,   8,   7,   7,   7,   7,   7,   7,   7,\n",
       "          7,   7,   7,   6,   6,   6,   6,   6,   6,   6,   6,   5,\n",
       "          5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,   5,\n",
       "          4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,   4,\n",
       "          4,   3,   3,   3,   3,   2,   2,   2,   2,   2,   2,   2,\n",
       "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1], dtype=torch.int16)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_l.type(torch.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = s_idx.view(-1,1,1)#.expand_as(x.unsqueeze(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  71]],\n",
       "\n",
       "        [[  13]],\n",
       "\n",
       "        [[  47]],\n",
       "\n",
       "        [[  49]],\n",
       "\n",
       "        [[  94]],\n",
       "\n",
       "        [[  69]],\n",
       "\n",
       "        [[ 142]],\n",
       "\n",
       "        [[ 109]],\n",
       "\n",
       "        [[  65]],\n",
       "\n",
       "        [[  89]],\n",
       "\n",
       "        [[ 125]],\n",
       "\n",
       "        [[ 138]],\n",
       "\n",
       "        [[  43]],\n",
       "\n",
       "        [[ 101]],\n",
       "\n",
       "        [[  95]],\n",
       "\n",
       "        [[ 120]],\n",
       "\n",
       "        [[  18]],\n",
       "\n",
       "        [[  72]],\n",
       "\n",
       "        [[  21]],\n",
       "\n",
       "        [[   8]],\n",
       "\n",
       "        [[  51]],\n",
       "\n",
       "        [[  30]],\n",
       "\n",
       "        [[   0]],\n",
       "\n",
       "        [[  35]],\n",
       "\n",
       "        [[  11]],\n",
       "\n",
       "        [[  93]],\n",
       "\n",
       "        [[  87]],\n",
       "\n",
       "        [[   5]],\n",
       "\n",
       "        [[  70]],\n",
       "\n",
       "        [[  10]],\n",
       "\n",
       "        [[ 156]],\n",
       "\n",
       "        [[ 103]],\n",
       "\n",
       "        [[ 130]],\n",
       "\n",
       "        [[  84]],\n",
       "\n",
       "        [[ 149]],\n",
       "\n",
       "        [[  42]],\n",
       "\n",
       "        [[  74]],\n",
       "\n",
       "        [[ 110]],\n",
       "\n",
       "        [[  14]],\n",
       "\n",
       "        [[   1]],\n",
       "\n",
       "        [[  16]],\n",
       "\n",
       "        [[  40]],\n",
       "\n",
       "        [[ 140]],\n",
       "\n",
       "        [[ 119]],\n",
       "\n",
       "        [[  81]],\n",
       "\n",
       "        [[  92]],\n",
       "\n",
       "        [[ 123]],\n",
       "\n",
       "        [[  75]],\n",
       "\n",
       "        [[  26]],\n",
       "\n",
       "        [[  34]],\n",
       "\n",
       "        [[   7]],\n",
       "\n",
       "        [[ 129]],\n",
       "\n",
       "        [[ 147]],\n",
       "\n",
       "        [[ 108]],\n",
       "\n",
       "        [[ 134]],\n",
       "\n",
       "        [[ 115]],\n",
       "\n",
       "        [[  82]],\n",
       "\n",
       "        [[  53]],\n",
       "\n",
       "        [[  15]],\n",
       "\n",
       "        [[  32]],\n",
       "\n",
       "        [[  31]],\n",
       "\n",
       "        [[ 111]],\n",
       "\n",
       "        [[  38]],\n",
       "\n",
       "        [[  59]],\n",
       "\n",
       "        [[ 154]],\n",
       "\n",
       "        [[ 152]],\n",
       "\n",
       "        [[  85]],\n",
       "\n",
       "        [[  96]],\n",
       "\n",
       "        [[  60]],\n",
       "\n",
       "        [[   6]],\n",
       "\n",
       "        [[ 158]],\n",
       "\n",
       "        [[  55]],\n",
       "\n",
       "        [[  78]],\n",
       "\n",
       "        [[ 135]],\n",
       "\n",
       "        [[ 150]],\n",
       "\n",
       "        [[ 127]],\n",
       "\n",
       "        [[ 144]],\n",
       "\n",
       "        [[ 141]],\n",
       "\n",
       "        [[ 136]],\n",
       "\n",
       "        [[  44]],\n",
       "\n",
       "        [[ 116]],\n",
       "\n",
       "        [[  80]],\n",
       "\n",
       "        [[ 100]],\n",
       "\n",
       "        [[  50]],\n",
       "\n",
       "        [[  23]],\n",
       "\n",
       "        [[  24]],\n",
       "\n",
       "        [[   3]],\n",
       "\n",
       "        [[ 104]],\n",
       "\n",
       "        [[  54]],\n",
       "\n",
       "        [[  79]],\n",
       "\n",
       "        [[  52]],\n",
       "\n",
       "        [[ 145]],\n",
       "\n",
       "        [[  56]],\n",
       "\n",
       "        [[  41]],\n",
       "\n",
       "        [[  45]],\n",
       "\n",
       "        [[ 159]],\n",
       "\n",
       "        [[  67]],\n",
       "\n",
       "        [[  39]],\n",
       "\n",
       "        [[ 146]],\n",
       "\n",
       "        [[ 118]],\n",
       "\n",
       "        [[  90]],\n",
       "\n",
       "        [[  62]],\n",
       "\n",
       "        [[  29]],\n",
       "\n",
       "        [[  98]],\n",
       "\n",
       "        [[ 148]],\n",
       "\n",
       "        [[  27]],\n",
       "\n",
       "        [[  61]],\n",
       "\n",
       "        [[  57]],\n",
       "\n",
       "        [[   2]],\n",
       "\n",
       "        [[ 126]],\n",
       "\n",
       "        [[ 128]],\n",
       "\n",
       "        [[ 157]],\n",
       "\n",
       "        [[  33]],\n",
       "\n",
       "        [[  97]],\n",
       "\n",
       "        [[  12]],\n",
       "\n",
       "        [[ 153]],\n",
       "\n",
       "        [[  83]],\n",
       "\n",
       "        [[  58]],\n",
       "\n",
       "        [[  88]],\n",
       "\n",
       "        [[  17]],\n",
       "\n",
       "        [[  46]],\n",
       "\n",
       "        [[  76]],\n",
       "\n",
       "        [[  28]],\n",
       "\n",
       "        [[  68]],\n",
       "\n",
       "        [[  37]],\n",
       "\n",
       "        [[  36]],\n",
       "\n",
       "        [[ 122]],\n",
       "\n",
       "        [[  91]],\n",
       "\n",
       "        [[ 121]],\n",
       "\n",
       "        [[  66]],\n",
       "\n",
       "        [[   4]],\n",
       "\n",
       "        [[  77]],\n",
       "\n",
       "        [[  73]],\n",
       "\n",
       "        [[ 105]],\n",
       "\n",
       "        [[ 151]],\n",
       "\n",
       "        [[ 113]],\n",
       "\n",
       "        [[ 107]],\n",
       "\n",
       "        [[ 137]],\n",
       "\n",
       "        [[  22]],\n",
       "\n",
       "        [[ 155]],\n",
       "\n",
       "        [[ 131]],\n",
       "\n",
       "        [[ 124]],\n",
       "\n",
       "        [[  64]],\n",
       "\n",
       "        [[ 112]],\n",
       "\n",
       "        [[   9]],\n",
       "\n",
       "        [[  20]],\n",
       "\n",
       "        [[ 106]],\n",
       "\n",
       "        [[  63]],\n",
       "\n",
       "        [[ 143]],\n",
       "\n",
       "        [[ 114]],\n",
       "\n",
       "        [[  48]],\n",
       "\n",
       "        [[ 102]],\n",
       "\n",
       "        [[ 132]],\n",
       "\n",
       "        [[  19]],\n",
       "\n",
       "        [[  86]],\n",
       "\n",
       "        [[ 117]],\n",
       "\n",
       "        [[ 133]],\n",
       "\n",
       "        [[  99]],\n",
       "\n",
       "        [[ 139]],\n",
       "\n",
       "        [[  25]]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "slice indices must be integers or None or have an __index__ method",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-efe747222279>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/onnx/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# fast pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmight_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mflat_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter_tensors_permissive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/utils/rnn.py\u001b[0m in \u001b[0;36mpack_padded_sequence\u001b[0;34m(input, lengths, batch_first)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackPadded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/packing.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, input, lengths, batch_first)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mprev_l\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mc_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                 \u001b[0msteps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprev_l\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mc_batch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m                 \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc_batch_size\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mprev_l\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mprev_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: slice indices must be integers or None or have an __index__ method"
     ]
    }
   ],
   "source": [
    "torch.nn.utils.rnn.pack_padded_sequence(s_i, s_l.cpu().data.numpy(), batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "max_length = 3\n",
    "hidden_size = 2\n",
    "n_layers =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# container\n",
    "batch_in = torch.zeros((batch_size, 1, max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0.]],\n",
       "\n",
       "        [[ 0.,  0.,  0.]]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data\n",
    "vec_1 = torch.FloatTensor([[1, 2, 3]])\n",
    "vec_2 = torch.FloatTensor([[1, 2, 0]])\n",
    "vec_3 = torch.FloatTensor([[1, 0, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  2.,  3.]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_in[0] = vec_1\n",
    "batch_in[1] = vec_2\n",
    "batch_in[2] = vec_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "batch_in = Variable(batch_in)\n",
    "\n",
    "seq_lengths = [3,2,1] # list of integers holding information about the batch size at each sequence step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-ab176b24b022>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# pack it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/onnx/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# fast pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmight_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mflat_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter_tensors_permissive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/utils/rnn.py\u001b[0m in \u001b[0;36mpack_padded_sequence\u001b[0;34m(input, lengths, batch_first)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackPadded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/packing.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, input, lengths, batch_first)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mprev_l\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mc_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                 \u001b[0msteps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprev_l\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mc_batch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m                 \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc_batch_size\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mprev_l\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0mprev_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "# pack it\n",
    "pack = torch.nn.utils.rnn.pack_padded_sequence(batch_in, seq_lengths, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.6",
   "language": "python",
   "name": "python3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
